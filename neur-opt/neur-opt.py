#!/usr/bin/env python3
"""
è¶…ç°¡åŒ–çš„ç¥ç¶“å…ƒå„ªåŒ–è…³æœ¬ - ä½¿ç”¨ç°¡åŒ–çš„ JSON æå–æ–¹æ³•
"""

import sys
import json
import argparse
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any
import re


WORKSPACE = Path.home() / ".openclaw" / "workspace"
MEMORY_DIR = WORKSPACE / "memory"
KB_FILE = WORKSPACE / "knowledge-base.md"
OLLAMA_URL = "http://localhost:11434"
MODEL = "ollama/qwen2.5:1.5b"


def load_daily_log(date_str: str = None) -> str:
    """è®€å–ä»Šæ—¥æ—¥èªŒ"""
    if date_str is None:
        date_str = datetime.now().strftime("%Y-%m-%d")
    
    log_file = MEMORY_DIR / f"{date_str}.md"
    
    if not log_file.exists():
        return f"# {date_str} - æ²’æœ‰æ´»å‹•è¨˜éŒ„\n\nä»Šå¤©é‚„æ²’æœ‰æ´»å‹•è¨˜éŒ„ã€‚"
    
    with open(log_file, 'r', encoding='utf-8') as f:
        return f.read()


def classify_entries(log_content: str) -> List[Dict[str, Any]]:
    """æå–æ—¥èªŒæ¢ç›®"""
    entries = []
    lines = log_content.split('\n')
    current_entry = []
    current_heading = ""
    
    for line in lines:
        if line.strip().startswith('##') or line.strip().startswith('###'):
            if current_entry:
                entries.append({
                    'heading': current_heading,
                    'content': '\n'.join(current_entry)
                })
            current_heading = line.strip()
            current_entry = []
        else:
            current_entry.append(line)
    
    if current_entry:
        entries.append({
            'heading': current_heading,
            'content': '\n'.join(current_entry)
        })
    
    if not entries:
        entries.append({
            'heading': "æ´»å‹•è¨˜éŒ„",
            'content': log_content
        })
    
    return entries


def classify_with_llm(entries: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """ä½¿ç”¨ LLM åˆ†é¡æ¯å€‹æ¢ç›® - ä½¿ç”¨ç°¡åŒ–çš„ prompt"""
    
    classified_entries = []
    
    for i, entry in enumerate(entries):
        heading = entry.get('heading', '')
        content = entry.get('content', '')
        
        # ç°¡åŒ–çš„ prompt - ç›´æ¥è¦æ±‚ JSONï¼Œä¸è¦å…¶ä»–è§£é‡‹
        prompt = f"""è«‹åˆ†æä»¥ä¸‹æ—¥èªŒæ¢ç›®ä¸¦è¿”å› JSONã€‚

åˆ†é¡é¸é …ï¼ˆé¸ä¸€å€‹ï¼‰ï¼š
- conversation
- task
- code
- system
- error
- research

æ—¥èªŒæ¨™é¡Œï¼š{heading}
æ—¥èªŒå…§å®¹ï¼š{content[:300]}

åªè¿”å› JSONï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{"category":"åˆ†é¡é¸é …"}}

åªè¿”å› JSONï¼Œä¸è¦ä»»ä½•å…¶ä»–å…§å®¹ã€è§£é‡‹æˆ–æ¨™é»ç¬¦è™Ÿã€‚"""
        
        try:
            result = call_ollama_llm_simple(prompt)
            
            # å˜—è©¦æå– JSON
            classification = extract_simple_json(result)
            
            if classification:
                classified_entries.append({
                    'original': entry,
                    'classification': classification
                })
            else:
                # ä½¿ç”¨é»˜èªåˆ†é¡
                classified_entries.append({
                    'original': entry,
                    'classification': {
                        'category': 'system',
                        'tags': 'uncategorized',
                        'summary': heading[:50]
                    }
                })
            
            # é¡¯ç¤ºé€²åº¦
            if (i + 1) % 5 == 0:
                print(f"   å·²è™•ç† {i + 1}/{len(entries)} å€‹æ¢ç›®...")
                
        except Exception as e:
            print(f"âš ï¸  åˆ†é¡å¤±æ•—: {e}")
            classified_entries.append({
                'original': entry,
                'classification': {
                    'category': 'error',
                    'tags': 'parsing_error',
                    'summary': str(e)[:100]
                }
            })
    
    return classified_entries


def extract_simple_json(text: str) -> Dict[str, Any]:
    """æå–ç°¡å–®çš„ JSON"""
    # æŸ¥æ‰¾ {...} æ¨¡å¼
    json_match = re.search(r'\{[^{}]*(?:"[^"]*"[^{}]*[^}]*)*[^{}]*\}', text)
    
    if json_match:
        try:
            return json.loads(json_match.group())
        except:
            pass
    
    # å¦‚æœæ‰¾ä¸åˆ° JSONï¼ŒæŸ¥æ‰¾ "category":"xxx"
    category_match = re.search(r'"category"\s*:\s*"([^"]+)"', text)
    if category_match:
        return {'category': category_match.group(1).strip('"')}
    
    return {}


def call_ollama_llm_simple(prompt: str) -> str:
    """èª¿ç”¨ Ollama LLM - ç°¡åŒ–ç‰ˆæœ¬"""
    import subprocess
    
    payload = {
        "model": MODEL.replace('ollama/', ''),
        "prompt": prompt,
        "stream": False,
        "raw": True,  # åªè¿”å›æ–‡æœ¬ï¼Œä¸åŒ…å«æ¨™è¨˜
        "options": {
            "temperature": 0.1,  # æ›´ä½çš„æº«åº¦ï¼Œä½¿è¼¸å‡ºæ›´ç¢ºå®š
            "max_tokens": 50,   # æ›´å°‘çš„ tokens
            "num_predict": 50
        }
    }
    
    result = subprocess.run(
        ['curl', '-s', '-X', 'POST', f'{OLLAMA_URL}/api/generate',
         '-H', 'Content-Type: application/json',
         '-d', json.dumps(payload)],
        capture_output=True,
        text=True,
        timeout=30
    )
    
    return result.stdout.strip()


def build_knowledge_base(classified_entries: List[Dict[str, Any]], kb_file: Path):
    """æ§‹å»ºçŸ¥è­˜åº«"""
    
    if kb_file.exists():
        with open(kb_file, 'r', encoding='utf-8') as f:
            kb_content = f.read()
    else:
        kb_content = "# çŸ¥è­˜åº«\n\n"
    
    # æŒ‰é¡åˆ¥åˆ†çµ„
    categorized = {}
    for entry in classified_entries:
        cat = entry['classification'].get('category', 'uncategorized')
        if cat not in categorized:
            categorized[cat] = []
        categorized[cat].append(entry)
    
    # æ§‹å»ºæ–°å…§å®¹
    new_kb = []
    new_kb.append(f"## {datetime.now().strftime('%Y-%m-%d')}")
    new_kb.append("")
    
    for category, entries in categorized.items():
        new_kb.append(f"### {category.upper()}")
        new_kb.append("")
        
        for entry in entries:
            original = entry['original']
            classification = entry['classification']
            
            heading = original.get('heading', 'ç„¡æ¨™é¡Œ')
            category_val = classification.get('category', '')
            summary = classification.get('summary', heading[:50])
            
            new_kb.append(f"#### {heading}")
            new_kb.append(f"**æ‘˜è¦ï¼š** {summary}")
            new_kb.append(f"**åˆ†é¡ï¼š** {category_val}")
            new_kb.append("")
    
    # åˆä½µ
    kb_lines = kb_content.split('\n')
    
    # ä¿ç•™æœ€è¿‘ 30 å¤©
    last_date_idx = -1
    for i, line in enumerate(kb_lines):
        if line.startswith('## 20'):
            last_date_idx = i
    
    if last_date_idx > 0:
        old_kb_lines = kb_lines[:last_date_idx]
    else:
        old_kb_lines = []
    
    full_kb = '\n'.join(old_kb_lines) + '\n' + '\n'.join(new_kb)
    
    # ä¿å­˜
    with open(kb_file, 'w', encoding='utf-8') as f:
        f.write(full_kb)
    
    print(f"âœ“ çŸ¥è­˜åº«å·²æ›´æ–°: {len(categorized)} å€‹é¡åˆ¥, {len(classified_entries)} å€‹æ¢ç›®")


def update_memory_link(classified_entries: List[Dict[str, Any]]):
    """æ›´æ–° MEMORY.md"""
    
    memory_file = WORKSPACE / "MEMORY.md"
    
    if not memory_file.exists():
        return
    
    with open(memory_file, 'r', encoding='utf-8') as f:
        memory_content = f.read()
    
    memory_lines = memory_content.split('\n')
    
    # æŸ¥æ‰¾çŸ¥è­˜åº«éƒ¨åˆ†
    kb_section_idx = -1
    for i, line in enumerate(memory_lines):
        if '## çŸ¥è­˜åº«' in line or '# çŸ¥è­˜åº«' in line:
            kb_section_idx = i
            break
    
    if kb_section_idx == -1:
        memory_lines.append("")
        memory_lines.append("## çŸ¥è­˜åº«")
        memory_lines.append(f"- é€£çµçŸ¥è­˜åº«: {KB_FILE}")
        memory_lines.append(f"- æœ€å¾Œæ›´æ–°: {datetime.now().strftime('%Y-%m-%d %H:%M')}")
        memory_lines.append("")
        
        with open(memory_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(memory_lines))


def main():
    parser = argparse.ArgumentParser(description='ç¥ç¶“å…ƒå„ªåŒ– - ç°¡åŒ–ç‰ˆ')
    parser.add_argument('--dry-run', action='store_true', help='è©¦é‹è¡Œï¼Œä¸ä¿®æ”¹æ–‡ä»¶')
    args = parser.parse_args()
    
    print("ğŸ§  ç¥ç¶“å…ƒå„ªåŒ–ï¼ˆNeural Optimizationï¼‰- ç°¡åŒ–ç‰ˆ")
    print(f"ğŸ“… æ—¥æœŸ: {datetime.now().strftime('%Y-%m-%d')}")
    print("")
    
    # 1. è®€å–ä»Šæ—¥æ—¥èªŒ
    print("1ï¸âƒ£  è®€å–ä»Šæ—¥æ—¥èªŒ...")
    log_content = load_daily_log()
    print(f"   âœ“ æ—¥èªŒå·²åŠ è¼‰ ({len(log_content)} å­—ç¬¦)")
    print("")
    
    # 2. åˆ†é¡æ¢ç›®
    print("2ï¸âƒ£  åˆ†é¡æ—¥èªŒæ¢ç›®...")
    entries = classify_entries(log_content)
    print(f"   âœ“ ç™¼ç¾ {len(entries)} å€‹æ¢ç›®")
    print("")
    
    # 3. ä½¿ç”¨ LLM åˆ†é¡
    print("3ï¸âƒ£  ä½¿ç”¨ LLM åˆ†é¡ï¼ˆä½¿ç”¨ç°¡åŒ–æ–¹æ³•ï¼‰...")
    classified = classify_with_llm(entries)
    
    # é¡¯ç¤ºåˆ†é¡çµæœ
    category_counts = {}
    for entry in classified:
        cat = entry['classification'].get('category', 'uncategorized')
        category_counts[cat] = category_counts.get(cat, 0) + 1
    
    for cat, count in category_counts.items():
        print(f"   - {cat}: {count}")
    print(f"   âœ“ åˆ†é¡å®Œæˆ")
    print("")
    
    # 4. æ§‹å»ºçŸ¥è­˜åº«
    print("4ï¸âƒ£  æ§‹å»ºçŸ¥è­˜åº«...")
    if not args.dry_run:
        build_knowledge_base(classified, KB_FILE)
        update_memory_link(classified)
        print(f"   âœ“ çŸ¥è­˜åº«å·²æ›´æ–°: {KB_FILE}")
        print(f"   âœ“ MEMORY.md å·²é€£æ¥")
    else:
        print("   [è©¦é‹è¡Œ] è·³éçŸ¥è­˜åº«æ›´æ–°")
    print("")
    
    # 5. ç”Ÿæˆæ‘˜è¦
    print("5ï¸âƒ£  ç”Ÿæˆæ‘˜è¦...")
    summary = {
        "date": datetime.now().strftime('%Y-%m-%d'),
        "total_entries": len(entries),
        "categories": category_counts,
        "knowledge_base": str(KB_FILE),
        "timestamp": datetime.now().isoformat()
    }
    
    print(f"   âœ“ æ‘˜è¦å·²ç”Ÿæˆ:")
    print(f"      - æ—¥æœŸ: {summary['date']}")
    print(f"      - ç¸½æ¢ç›®: {summary['total_entries']}")
    print(f"      - åˆ†é¡: {summary['categories']}")
    print("")
    
    # ä¿å­˜æ‘˜è¦
    summary_dir = WORKSPACE / "neur-opt"
    summary_dir.mkdir(parents=True, exist_ok=True)
    summary_file = summary_dir / f"summary-{datetime.now().strftime('%Y-%m-%d')}.json"
    
    if not args.dry_run:
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        print(f"   âœ“ æ‘˜è¦å·²ä¿å­˜: {summary_file}")
    else:
        print("   [è©¦é‹è¡Œ] è·³éæ‘˜è¦ä¿å­˜")
    print("")
    
    print("âœ¨ ç¥ç¶“å…ƒå„ªåŒ–å®Œæˆï¼")


if __name__ == "__main__":
    main()
